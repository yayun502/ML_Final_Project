{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from feature_engine.encoding import WoEEncoder\n",
    "import pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set datasets: target(y_train) / train(X_train) / data(X_train + X_test) \n",
    "target = train['failure']\n",
    "train = train.drop('failure', axis=1)\n",
    "data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding missing values as extra columns => observe failure rate when feature is missing \n",
    "data['m3_missing'] = data['measurement_3'].isnull().astype(np.int8)\n",
    "data['m5_missing'] = data['measurement_5'].isnull().astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in test.columns if col.startswith('measurement')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float type columns => find 4 most correlated columns for measurement_3 ~ measurement_17\n",
    "col_corrsum = list()\n",
    "col_name = list()\n",
    "for x in range(3, 18):   \n",
    "    corr = np.absolute(data[cols].corr()[f'measurement_{x}']).sort_values(ascending=False)\n",
    "    col_corrsum.append(np.round(np.sum(corr[1:5]),5))\n",
    "    col_name.append(f'measurement_{x}')\n",
    "\n",
    "# 計算哪些欄位 和其他欄位有較大關聯性\n",
    "# create df for it\n",
    "c = pd.DataFrame()\n",
    "c['Selected columns'] = col_name\n",
    "c['correlation total'] = col_corrsum\n",
    "c = c.sort_values(by = 'correlation total', ascending=False).reset_index(drop = True)\n",
    "\n",
    "# 和其他欄位有較大關聯性的這些欄位 在各product code中關聯性表現如何 \n",
    "# create dictionary for it => convenient for looking up\n",
    "dict = {}\n",
    "for i in range(8):\n",
    "    selected_col = c.loc[i, 'Selected columns']\n",
    "    selected_col_dic = {}\n",
    "    for pc in data.product_code.unique(): \n",
    "        corr = np.absolute(data.loc[data.product_code == pc, cols].corr()[selected_col]).sort_values(ascending=False)\n",
    "        # print(f'product code: {x}, selected columns: {selected_col}, corr: ')\n",
    "        # print(corr)\n",
    "        selected_col_dic[pc] = list(corr[1:5].index)\n",
    "    dict[selected_col] = selected_col_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in data.columns if col.startswith('measurement') or col=='loading']\n",
    "null_cols = [col for col in train.columns if train[col].isnull().sum()!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#filled (by linear model) = 1499\n",
      "#filled (by KNN) = 3829\n"
     ]
    }
   ],
   "source": [
    "# fill NaN/null value \n",
    "for pc in data.product_code.unique():\n",
    "    \n",
    "    num_NaN_filled_byModel = 0\n",
    "    num_NaN_filled_byKNN = 0\n",
    "\n",
    "    # print(f'Product code {pc}: filled by linear model (HuberRegressor):')\n",
    "    for selected_col in dict.keys():\n",
    "        # try to fill NaN (if meeting requirements about features needed) => use those to train a model \n",
    "        data_with_specific_pc = data[data.product_code == pc]\n",
    "        features_train = [selected_col] + dict[selected_col][pc]   # dict[selected_col][pc]=look up dictionary(for selected_col performing in the specific product code)\n",
    "        L_train = data_with_specific_pc[features_train].dropna(axis=0, how='any')  # use features_train to train linear model(also drop NaN row)\n",
    "        LX_train = L_train[dict[selected_col][pc]]\n",
    "        Ly_train = L_train[selected_col]\n",
    "        subdata_pc = data[data.product_code == pc][features_train]\n",
    "        condition1 = (subdata_pc[dict[selected_col][pc]].isnull().sum(axis=1) == 0)  # axis=1 : cross columns\n",
    "        condition2 = (subdata_pc[selected_col].isnull())\n",
    "        LX_test = subdata_pc[condition1 & condition2]\n",
    "        # print(len(LX_train), len(LX_test))\n",
    "\n",
    "        if(len(LX_train)!=0 and len(LX_test)!=0):\n",
    "            model = HuberRegressor(epsilon=1.35, max_iter=400)\n",
    "            model.fit(LX_train, Ly_train)\n",
    "            prediction = model.predict(data[data.product_code == pc].loc[LX_test.index.tolist(), dict[selected_col][pc]])\n",
    "            data[data.product_code == pc].loc[LX_test.index.tolist(), selected_col]= prediction\n",
    "            # print(f\"fill '{selected_col}': #row = {len(LX_test)}\")\n",
    "            num_NaN_filled_byModel += len(LX_test)\n",
    "        \n",
    "    # columns still NaN/null (result from not meeting requirements about features needed) => use 'features' to impute\n",
    "    num_NaN_filled_byKNN = data.loc[data[\"product_code\"] == pc, null_cols].isnull().sum().sum()\n",
    "    knn = KNNImputer(n_neighbors=3)\n",
    "    data.loc[data.product_code==pc, features] = knn.fit_transform(data.loc[data.product_code==pc, features])\n",
    "\n",
    "print(f'#filled (by linear model) = {num_NaN_filled_byModel}') \n",
    "print(f'#filled (by KNN) = {num_NaN_filled_byKNN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload train/test because preprocessing\n",
    "train = data.iloc[:len(train),:]\n",
    "test = data.iloc[len(train):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train\n",
    "y = target\n",
    "# labeling for 'attribute_0' => use WoEEncoder\n",
    "woe_encoder = WoEEncoder(variables=['attribute_0'])\n",
    "woe_encoder.fit(X, y)\n",
    "X = woe_encoder.transform(X)\n",
    "test = woe_encoder.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick based on correlation analysis and important_list \n",
    "features_for_training = ['loading',\n",
    "                        'attribute_0',\n",
    "                        'measurement_17',\n",
    "                        'measurement_0',\n",
    "                        'measurement_1',\n",
    "                        'measurement_2',\n",
    "                        'm3_missing',\n",
    "                        'm5_missing',\n",
    "                        # 'attribute_3',\n",
    "                        # 'measurement_4',\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling for the certain features columns \n",
    "def scaling_specific_features(df_list, features):\n",
    "    \n",
    "    df_new_list = []\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        # fit and transform\n",
    "        if i==0:\n",
    "            df_scaled = scaler.fit_transform(df[features])\n",
    "        else:\n",
    "            df_scaled = scaler.transform(df[features])\n",
    "        # put it back\n",
    "        df_new = df.copy()\n",
    "        df_new[features] = df_scaled\n",
    "        df_new_list.append(df_new)\n",
    "    \n",
    "    return df_new_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = pickle.load(open('model_best_publicScore.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20192883, 0.19422158, 0.19818543, ..., 0.18903897, 0.21466629,\n",
       "       0.19475708])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test = scaling_specific_features([X, test], features_for_training)\n",
    "prediction_full = model.predict_proba(x_test[features_for_training])[:, 1]\n",
    "\n",
    "prediction_full"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output csv file\n",
    "with open('submission_best_publicScore_pickle.csv', 'w', newline='') as csvfile:\n",
    "  csv_writer = csv.writer(csvfile)\n",
    "  csv_writer.writerow([\"id\", \"failure\"])\n",
    "  for id, value in zip(list(test.id), prediction_full):\n",
    "    csv_writer.writerow([id, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6a0ffad3a294a4eb8b29a8c17e7671d526bb3fd9e88c0a2803a38bee8b75c21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
